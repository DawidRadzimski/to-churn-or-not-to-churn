{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-15T07:14:47.825989Z",
     "start_time": "2024-08-15T07:14:47.650137Z"
    }
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load the data\n",
    "print(\"Loading data...\")\n",
    "data = pd.read_csv('./data/internet_service_churn.csv')\n",
    "\n",
    "# 2. Handling Missing Data\n",
    "print(\"Processing missing data...\")\n",
    "\n",
    "def process_missing_data(df, threshold=3):\n",
    "    \"\"\"\n",
    "    Function to process missing data.\n",
    "    If a row has 3 or fewer missing values, they are filled with mean values.\n",
    "    If a row has more than 3 missing values, it is dropped.\n",
    "    \"\"\"\n",
    "    # Drop rows where the number of missing values exceeds the threshold\n",
    "    df.dropna(thresh=len(df.columns) - threshold, inplace=True)\n",
    "    \n",
    "    # Fill remaining missing values with the mean of each column\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[:] = imputer.fit_transform(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process missing data\n",
    "data = process_missing_data(data)\n",
    "\n",
    "# Reset index after dropping rows\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Data after processing missing values:\")\n",
    "print(data.info())\n",
    "\n",
    "# 3. Data Preparation for Modeling\n",
    "\n",
    "# Splitting data into features (X) and target variable (y)\n",
    "X = data.drop(columns=['churn'])\n",
    "y = data['churn']\n",
    "\n",
    "# Encoding categorical variables (if any) before converting to NumPy arrays\n",
    "label_encoders = {}\n",
    "for column in X.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    X[column] = label_encoders[column].fit_transform(X[column])\n",
    "\n",
    "# Now convert X and y to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Ensure that X and y are of the correct data types\n",
    "print(\"Data types after conversion:\")\n",
    "print(f\"X dtype: {X.dtype}\")\n",
    "print(f\"y dtype: {y.dtype}\")\n",
    "\n",
    "# Convert to specific data types if necessary\n",
    "X = X.astype(np.float64)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# Normalizing features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Train-Test Split\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Building the Neural Network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Training the Model\n",
    "\n",
    "print(\"Training the model...\")\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 7. Evaluating the Model\n",
    "\n",
    "print(\"Evaluating the model...\")\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred_train = (y_pred_train > 0.5).astype(int)\n",
    "y_pred_test = (y_pred_test > 0.5).astype(int)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_type=\"Test\"):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"{dataset_type} Set Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title(f'Confusion Matrix - {dataset_type} Set')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate on training set\n",
    "evaluate_model(y_train, y_pred_train, \"Training\")\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluate_model(y_test, y_pred_test)\n",
    "\n",
    "# 8. Plotting Training History\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Model Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing missing data...\n",
      "Data after processing missing values:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72274 entries, 0 to 72273\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   id                           72274 non-null  int64  \n",
      " 1   is_tv_subscriber             72274 non-null  int64  \n",
      " 2   is_movie_package_subscriber  72274 non-null  int64  \n",
      " 3   subscription_age             72274 non-null  float64\n",
      " 4   bill_avg                     72274 non-null  int64  \n",
      " 5   reamining_contract           72274 non-null  float64\n",
      " 6   service_failure_count        72274 non-null  int64  \n",
      " 7   download_avg                 72274 non-null  float64\n",
      " 8   upload_avg                   72274 non-null  float64\n",
      " 9   download_over_limit          72274 non-null  int64  \n",
      " 10  churn                        72274 non-null  int64  \n",
      "dtypes: float64(4), int64(7)\n",
      "memory usage: 6.1 MB\n",
      "None\n",
      "Data types after conversion:\n",
      "X dtype: float64\n",
      "y dtype: int64\n",
      "Splitting data into training and testing sets...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dawidradzimski/.local/share/virtualenvs/to-churn-or-not-to-churn-7On2Y9Xh/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 98\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# 6. Training the Model\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining the model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 98\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# 7. Evaluating the Model\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating the model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/to-churn-or-not-to-churn-7On2Y9Xh/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/to-churn-or-not-to-churn-7On2Y9Xh/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py:108\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m    106\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[1;32m    107\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m--> 108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9d00af7c949e8569"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
